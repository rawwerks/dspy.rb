---
layout: page
title: "RLM — Iterative Code Interpreter"
description: "DSPy::RLM gives an LLM a Ruby REPL. It generates code, executes it, observes the output, and iterates until it calls SUBMIT with the final answer."
---

# DSPy::RLM

RLM (Reasoning, Learning, and Memory through code) is a DSPy module that gives an LLM a sandboxed Ruby REPL. Instead of answering in one shot, the model writes code, executes it, reads the output, and iterates — exploring data programmatically before committing a final answer.

This is particularly effective for tasks involving large inputs (long documents, conversation histories, structured data) where a single LLM call would require the model to process everything in its context window at once.

## How it works

```
┌─────────────┐     ┌───────────────┐     ┌─────────────┐
│  LLM generates  │───▶│  Ruby REPL     │───▶│  Observe     │
│  reasoning + code│     │  executes code │     │  output      │──┐
└─────────────┘     └───────────────┘     └─────────────┘  │
       ▲                                                       │
       └───────────── loop until SUBMIT() or max iterations ───┘
```

Each iteration:

1. **Generate action** — the LLM sees variable metadata + REPL history and produces reasoning + Ruby code
2. **Execute** — the code runs in a sandboxed `RubyREPL` with access to input variables, `llm_query()`, and user tools
3. **Observe** — stdout and the eval result are appended to the REPL history
4. **Repeat or finish** — if the code called `SUBMIT(field: value)`, the loop ends and a `Prediction` is returned. Otherwise, the model gets another turn.

If `max_iterations` is hit without a `SUBMIT`, an extract-fallback predictor reads the full REPL history and produces the output directly.

## Quick start

```ruby
require 'dspy'

DSPy.configure do |c|
  c.lm = DSPy::LM.new('openai/gpt-4o-mini', api_key: ENV['OPENAI_API_KEY'])
end

class ContextQA < DSPy::Signature
  description "Answer a question by writing Ruby code to analyze the context."

  input do
    const :context, String, description: "Text passage to analyze"
    const :question, String, description: "Question to answer"
  end

  output do
    const :answer, String, description: "The answer to the question"
  end
end

rlm = DSPy::RLM.new(ContextQA, max_iterations: 10, verbose: true)

result = rlm.call(
  context: "Ruby was created by Yukihiro Matsumoto and first released in 1995.",
  question: "Who created Ruby?"
)

puts result.answer  # => "Yukihiro Matsumoto"
```

## Configuration

| Parameter | Default | Description |
|---|---|---|
| `max_iterations` | `20` | Maximum REPL turns before extract fallback |
| `max_llm_calls` | `50` | Maximum `llm_query()` calls allowed in the sandbox |
| `max_output_chars` | `10_000` | Truncation limit for REPL output in history |
| `timeout` | `nil` | Wall-clock seconds limit for the entire `forward()` call |
| `verbose` | `false` | Print REPL interactions to stderr |
| `tools` | `[]` | User-provided tools (Hash or Array — see [Tools](#tools)) |
| `sub_lm` | `nil` | Separate LM for `llm_query()` calls (defaults to `DSPy.config.lm`) |
| `interpreter` | `nil` | Custom `CodeInterpreter` instance (defaults to `RubyREPL`) |

## SUBMIT

The sandbox exposes a `SUBMIT` function whose keyword arguments must match the signature's output fields. Calling it ends the loop immediately.

```ruby
# Inside the sandbox (generated by the LLM):
SUBMIT(answer: "Yukihiro Matsumoto")
```

If the LLM calls `SUBMIT` with missing fields, the error is appended to the REPL history and the model gets another turn to fix it. Type coercion is applied automatically — if the output field is `Integer`, a string `"42"` will be coerced to `42`.

## llm_query

Every RLM sandbox includes `llm_query(prompt)` — a function that calls a sub-LLM for semantic analysis. This lets the model delegate reasoning-heavy sub-tasks (summarization, classification, inference) while using Ruby for data manipulation (search, filter, aggregate).

```ruby
# Inside the sandbox (generated by the LLM):
# Find relevant paragraphs with Ruby, then ask the LLM to interpret them
matches = context.lines.select { |l| l.include?("created") }
answer = llm_query("Based on: #{matches.join}\nWho created Ruby?")
SUBMIT(answer: answer)
```

The call count is capped by `max_llm_calls`. By default `llm_query` uses `DSPy.config.lm`, but you can pass a cheaper/faster model via the `sub_lm:` parameter.

## Tools

Pass custom tools as a Hash of name → callable:

```ruby
search_tool = lambda { |query| ExternalAPI.search(query) }

rlm = DSPy::RLM.new(
  MySignature,
  tools: { 'search' => search_tool }
)
```

Tools are callable by name inside the sandbox:

```ruby
# Inside the sandbox:
results = search("Ruby programming language")
puts results
```

Array input is also supported — objects with `tool_name` and `call` methods are auto-registered.

## Testing with MockREPL

`MockREPL` lets you script interpreter responses without executing real code. This is useful for testing the RLM loop logic without an LLM.

```ruby
require 'dspy/interpreters/mock_repl'

mock = DSPy::Interpreters::MockREPL.new(
  responses: [
    "found 3 matches",                              # iteration 1 output
    DSPy::FinalOutput.new({ answer: "Matsumoto" })   # iteration 2 → SUBMIT
  ]
)

rlm = DSPy::RLM.new(MySignature, interpreter: mock)
```

`MockREPL` records all code it receives in `executed_code` and tracks `call_count`, so you can assert on what the LLM generated.

## The sandbox

`RubyREPL` provides a sandboxed Ruby binding with:

- **State persistence** — variables and method definitions survive across iterations
- **Stdout capture** — `puts`/`print` output is captured, not leaked to the real stdout
- **Code fence stripping** — markdown fences (` ```ruby `) are removed automatically
- **Safe requires** — only stdlib modules (`json`, `set`, `date`, `uri`, `csv`, `digest`, `base64`, `net/http`, etc.)
- **Error wrapping** — `RuntimeError`, `SyntaxError`, and other exceptions become `CodeInterpreterError` messages in the REPL history, giving the LLM a chance to self-correct

## LongMemEval benchmark

The `examples/longmemeval_runner.rb` script runs RLM against the [LongMemEval](https://arxiv.org/abs/2407.04659) QA benchmark — questions about long conversation histories that require searching through haystack sessions.

```bash
# Smoke test (5 questions)
ruby examples/longmemeval_runner.rb --data data/sample.json --limit 5

# Full run with resume support
ruby examples/longmemeval_runner.rb \
  --data data/longmemeval.json \
  --output results.jsonl \
  --max-iterations 15 \
  --resume

# Use a specific model
ruby examples/longmemeval_runner.rb \
  --data data/longmemeval.json \
  --model openai/gpt-4o \
  --verbose
```

Results are appended to a JSONL file. Use `--resume` to skip already-completed question IDs, making runs resumable across interruptions.

## Architecture

```
DSPy::RLM
├── generate_action   — Predict module (inputs: variables_info, repl_history, iteration → outputs: reasoning, code)
├── extract           — Predict module (fallback: reads history → produces user's output fields)
├── RubyREPL          — sandboxed interpreter (or MockREPL for tests)
├── REPLHistory       — immutable append-only log of (reasoning, code, output) triples
└── REPLVariable      — metadata about each input variable (type, length, preview)
```

RLM builds two internal DSPy signatures dynamically from your signature class. The action signature's system prompt includes REPL instructions, available tools, and the SUBMIT directive. The extract signature is used only as a fallback when `max_iterations` is reached.
